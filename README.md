# ğŸ“ˆ Simple Linear Regression

A foundational implementation of Simple Linear Regression for predictive modeling and understanding linear relationships. This project demonstrates how to build, train, and evaluate regression models with a single independent variable to predict continuous target values.

## ğŸ“‹ Description

This project implements Simple Linear Regression, the most fundamental machine learning algorithm used to model the linear relationship between a single independent variable (feature) and a dependent variable (target). The implementation covers the entire machine learning pipeline including data preprocessing, model training, evaluation, and visualization to understand how one variable influences another.

## âœ¨ Features

- ğŸ“‰ **Single Variable Analysis**: Models relationship between one feature and target
- ğŸ“Š **Visual Insights**: Clear scatter plots with regression line visualization
- ğŸ”§ **Data Preprocessing**: Handles data cleaning and preparation
- ğŸ§  **Model Training**: Implements the Ordinary Least Squares (OLS) method
- ğŸ“ˆ **Performance Evaluation**: RÂ² score, MSE, RMSE metrics
- ğŸ“Š **Prediction Capability**: Make predictions on new data points
- ğŸ“ **Coefficient Analysis**: Understanding slope and intercept
- âš–ï¸ **Best-Fit Line**: Calculates optimal line through data points

## ğŸ› ï¸ Technologies Used

- **Python 3.x**
- **NumPy**: Numerical operations and calculations
- **Pandas**: Data manipulation and analysis
- **Scikit-learn**: Machine learning implementation
- **Matplotlib**: Data visualization and plotting
- **Seaborn**: Enhanced statistical visualizations
- **Jupyter Notebook**: Interactive development

## ğŸ“Š How Simple Linear Regression Works

Simple Linear Regression finds the best-fitting straight line through the data points:

**Formula**: `y = mx + b`

Or in statistical notation: `y = Î²â‚€ + Î²â‚x + Îµ`

Where:
- `y` = Dependent variable (what we're predicting)
- `x` = Independent variable (what we're using to predict)
- `Î²â‚€` (b) = Intercept (y-axis crossing point)
- `Î²â‚` (m) = Slope (rate of change)
- `Îµ` = Error term (residuals)

### The Algorithm

1. **Calculate the slope (m)**: `m = Î£[(xáµ¢ - xÌ„)(yáµ¢ - yÌ„)] / Î£[(xáµ¢ - xÌ„)Â²]`
2. **Calculate the intercept (b)**: `b = yÌ„ - m * xÌ„`
3. **Make predictions**: `yÌ‚ = mx + b`

## ğŸ“ Project Structure

```
simple-linear-regression/
â”‚
â”œâ”€â”€ main                     # Jupyter Notebook with full implementation
â”œâ”€â”€ main.py                  # Python script version
â”œâ”€â”€ LICENSE                  # MIT License
â””â”€â”€ README.md                # Project documentation
```

## ğŸš€ Installation & Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/lakumsaicharan/simple-linear-regression.git
   cd simple-linear-regression
   ```

2. **Install dependencies**:
   ```bash
   pip install numpy pandas scikit-learn matplotlib seaborn jupyter
   ```

3. **Run Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```

4. **Open and explore**:
   - Open `main` notebook in Jupyter
   - Execute cells to see the implementation
   - Alternatively, run `main.py` for command-line execution

## ğŸ“š Usage Example

### Basic Implementation

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Sample data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model coefficients
print(f'Slope (m): {model.coef_[0]}')
print(f'Intercept (b): {model.intercept_}')

# Model evaluation
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'RÂ² Score: {r2}')
print(f'RMSE: {rmse}')

# Visualize
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X, model.predict(X), color='red', label='Regression Line')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
```

## ğŸ“ˆ Key Concepts

### Evaluation Metrics

- **RÂ² Score (Coefficient of Determination)**: 
  - Range: 0 to 1 (higher is better)
  - Measures how well the model explains variance
  - RÂ² = 1 means perfect fit
  
- **MSE (Mean Squared Error)**:
  - Average of squared differences
  - Penalizes larger errors more
  
- **RMSE (Root Mean Squared Error)**:
  - Square root of MSE
  - Same units as the target variable
  - Easier to interpret

### Assumptions

1. **Linearity**: Relationship between X and y is linear
2. **Independence**: Observations are independent
3. **Homoscedasticity**: Constant variance of residuals
4. **Normality**: Residuals are normally distributed
5. **No outliers**: Extreme values can skew the line

## ğŸ“ Learning Objectives

This project demonstrates:
- âœ… Simple Linear Regression implementation
- âœ… Data visualization techniques
- âœ… Model training and evaluation
- âœ… Understanding slope and intercept
- âœ… Calculating best-fit line
- âœ… Interpreting regression coefficients
- âœ… Making predictions on new data

## ğŸ’¼ Real-World Applications

- **Sales Forecasting**: Predicting sales based on advertising spend
- **Temperature Conversion**: Converting Celsius to Fahrenheit
- **Salary Prediction**: Estimating salary based on years of experience
- **Stock Prices**: Basic trend analysis
- **Height vs Weight**: Understanding body mass relationships
- **Study Hours vs Grades**: Academic performance prediction
- **House Size vs Price**: Real estate valuation

## ğŸ“‰ When to Use Simple Linear Regression

**Best Used When:**
- âœ… You have one independent variable
- âœ… Relationship appears linear
- âœ… Quick exploratory analysis needed
- âœ… Baseline model for comparison

**Consider Alternatives When:**
- âŒ Multiple independent variables (use Multiple Linear Regression)
- âŒ Non-linear relationships (use Polynomial Regression)
- âŒ Categorical predictions (use Classification)

## ğŸ”§ Extending the Model

- Add data preprocessing pipelines
- Implement residual analysis
- Add confidence intervals
- Create interactive visualizations
- Handle outliers detection
- Add cross-validation

## ğŸ¤ Contributing

Contributions welcome! Feel free to:
- ğŸ› Report bugs or issues
- ğŸ’¡ Suggest improvements
- ğŸ”§ Submit pull requests
- ğŸ“Š Add new visualizations

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Lakum Sai Charan**
- GitHub: [@lakumsaicharan](https://github.com/lakumsaicharan)
- Part of the 100 Days of Code Challenge
- Machine Learning & Data Science Journey

## ğŸ™ Acknowledgments

- Foundation of supervised learning
- Built as part of ML fundamentals
- Thanks to the scikit-learn team
- Inspired by classical statistics

## ğŸ“š Resources

- [Scikit-learn Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [Understanding Linear Regression](https://en.wikipedia.org/wiki/Simple_linear_regression)
- [Statistics and ML Connection](https://www.statlearning.com/)

---

â­ **Found this useful? Star the repo!** â­

*Building predictive models one variable at a time* ğŸ“ˆ
